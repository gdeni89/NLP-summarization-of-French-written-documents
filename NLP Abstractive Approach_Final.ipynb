{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "42693139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset orange_sum (C:\\Users\\Giuseppe\\.cache\\huggingface\\datasets\\GEM___orange_sum\\abstract\\1.1.0\\0886904dd6ac7849d4c9fcec77bf8d78622dd651739826ccc8436159b777f910)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07632e4eaa5d41ec874535c1dec2a250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_orangesum = load_dataset(\"GEM/OrangeSum\", \"abstract\") # we can also specify \"title\" to obtain pairs of text-title\n",
    "#dataset_xlsum = load_dataset(\"csebuetnlp/xlsum\", \"french\")\n",
    "#dataset_mlsum = load_dataset(\"mlsum\", \"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3cdb38c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.13.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3306ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ac042a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):\n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms \n",
    "    #if True:\n",
    "     #   text = text.split()\n",
    "      #  new_text = []\n",
    "       # for word in text:\n",
    "        #    if word in contractions:\n",
    "         #       new_text.append(contractions[word])\n",
    "          #  else:\n",
    "           #     new_text.append(word)\n",
    "        #text = \" \".join(new_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"french\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bfd09a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (21401, 4), 'test': (1500, 4), 'validation': (1500, 4)}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_orangesum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9413abe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gem_id        0\n",
       "input         0\n",
       "target        0\n",
       "references    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_OS = pd.DataFrame(dataset_orangesum['test'])\n",
    "df_train_OS = pd.DataFrame(dataset_orangesum['train'])\n",
    "df_validation_OS = pd.DataFrame(dataset_orangesum['validation'])\n",
    "\n",
    "df_train_OS.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecaa036f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gem_id</th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "      <th>references</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OrangeSum_abstract-train-0</td>\n",
       "      <td>Thierry Mariani sur la liste du Rassemblement ...</td>\n",
       "      <td>L'information n'a pas été confirmée par l'inté...</td>\n",
       "      <td>[L'information n'a pas été confirmée par l'int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OrangeSum_abstract-train-1</td>\n",
       "      <td>C'est désormais officiel : Alain Juppé n'est p...</td>\n",
       "      <td>Le maire de Bordeaux ne fait plus partie des R...</td>\n",
       "      <td>[Le maire de Bordeaux ne fait plus partie des ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OrangeSum_abstract-train-2</td>\n",
       "      <td>La mesure est décriée par les avocats et les m...</td>\n",
       "      <td>En 2020, les tribunaux d'instance fusionnent a...</td>\n",
       "      <td>[En 2020, les tribunaux d'instance fusionnent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OrangeSum_abstract-train-3</td>\n",
       "      <td>Dans une interview accordée au Figaro mercredi...</td>\n",
       "      <td>Les médecins jugés \"gros prescripteurs d'arrêt...</td>\n",
       "      <td>[Les médecins jugés \"gros prescripteurs d'arrê...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OrangeSum_abstract-train-4</td>\n",
       "      <td>Le préjudice est estimé à 2 millions d'euros. ...</td>\n",
       "      <td>Il aura fallu mobiliser 90 gendarmes pour cett...</td>\n",
       "      <td>[Il aura fallu mobiliser 90 gendarmes pour cet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       gem_id  \\\n",
       "0  OrangeSum_abstract-train-0   \n",
       "1  OrangeSum_abstract-train-1   \n",
       "2  OrangeSum_abstract-train-2   \n",
       "3  OrangeSum_abstract-train-3   \n",
       "4  OrangeSum_abstract-train-4   \n",
       "\n",
       "                                               input  \\\n",
       "0  Thierry Mariani sur la liste du Rassemblement ...   \n",
       "1  C'est désormais officiel : Alain Juppé n'est p...   \n",
       "2  La mesure est décriée par les avocats et les m...   \n",
       "3  Dans une interview accordée au Figaro mercredi...   \n",
       "4  Le préjudice est estimé à 2 millions d'euros. ...   \n",
       "\n",
       "                                              target  \\\n",
       "0  L'information n'a pas été confirmée par l'inté...   \n",
       "1  Le maire de Bordeaux ne fait plus partie des R...   \n",
       "2  En 2020, les tribunaux d'instance fusionnent a...   \n",
       "3  Les médecins jugés \"gros prescripteurs d'arrêt...   \n",
       "4  Il aura fallu mobiliser 90 gendarmes pour cett...   \n",
       "\n",
       "                                          references  \n",
       "0  [L'information n'a pas été confirmée par l'int...  \n",
       "1  [Le maire de Bordeaux ne fait plus partie des ...  \n",
       "2  [En 2020, les tribunaux d'instance fusionnent ...  \n",
       "3  [Les médecins jugés \"gros prescripteurs d'arrê...  \n",
       "4  [Il aura fallu mobiliser 90 gendarmes pour cet...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_OS.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d596800",
   "metadata": {},
   "source": [
    "Removing stop words from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c0c5ab9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Giuseppe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries are complete.\n",
      "Texts are complete.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "  \n",
    "# Clean the summaries and texts\n",
    "clean_target = []\n",
    "for target in df_train_OS.target:\n",
    "    clean_target.append(clean_text(target, remove_stopwords=False))\n",
    "print(\"Summaries are complete.\")\n",
    "\n",
    "clean_input = []\n",
    "for input in df_train_OS.input:\n",
    "    clean_input.append(clean_text(input))\n",
    "print(\"Texts are complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "12349410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean News # 1\n",
      "l information n a pas été confirmée par l intéressé qui déclare toutefois étudier la question \n",
      "\n",
      "thierry mariani liste rassemblement national rn ex fn européennes affirme mardi 11 septembre chez pol nouvelle newsletter politique libération ancien député républicain ministre nicolas sarkozy point rejoindre troupes marine pen élections européennes 2019 ça va faire plus question calendrier obligé annoncer tout suite huit mois européennes ainsi assuré membre influent rn contacté franceinfo mariani a confirmé information élections juin sais numéro 1 liste a répondu ancien ministre transports reconnaît toutefois toujours cité franceinfo nom liste rn fait partie possibilités fréjus ville sympathique prévu rendre week end a ailleurs commenté twitter alors marine pen réunit cadres parti week end cité varoise proximité connue fnla proximité thierry mariani parti frontiste nouvelle sans alliés allons rester opposition longtemps temps renverser table front national a évolué regardons si accord rapprochement possibles déclaré interview donnée journal dimanche mars dernier puis avril bruissé rumeur rencontre entre ex député marine pen proposé figurer position éligible liste parti européennes conclusion hâtive époque écrit twitter mois thierry mariani cosigné tribune publiée valeurs actuelles côté élus frontistes appelant union droites\n",
      "\n",
      "Clean News # 2\n",
      "le maire de bordeaux ne fait plus partie des républicains et il tient à le montrer  lors de ses voeux à la presse  l édile a pris ses distances avec sa famille politique historique  qu il trouve trop proche de marine le pen \n",
      "\n",
      "désormais officiel alain juppé plus membre républicains ex premier ministre jacques chirac cofondateur ump 2002 paie plus cotisation auprès parti droite mercredi 9 janvier maire bordeaux a dénoncé glissement opère selon droite vers extême droite reconnais moins moins cette famille politique laquelle pourtant très attaché tristesse quittée a dérive vers thèses celles très proches extrême droite ambiguïté europe a déclaré face journalistes réunis assister voeux assiste cette espèce transfusion régulière thèmes fond a moments où demande entends radio membre lr rn a insisté maire bordeaux jour ex député thierry mariani annonçait départ lr rallier liste rassemblement national européennes mai prochain cela fait deux ans dit prenais distances républicains choses acquises depuis bien longtemps a tranché alain juppé ancien candidat primaire droite présidentielle 2017 a jamais fait mystère désaccord positions président républicains laurent wauquiez\n",
      "\n",
      "Clean News # 3\n",
      "en 2020  les tribunaux d instance fusionnent avec ceux de grande instance pour former un unique  tribunal judiciaire   c est la principale mesure de la réforme de la justice  portée par la garde des sceaux nicole belloubet \n",
      "\n",
      "mesure décriée avocats magistrats juridictions proximité excellence traitant litiges quotidien tribunaux instance apprêtent fusionner tribunaux grande instance cette réorganisation principales mesures réforme justice promulguée 23 mars professionnels inquiètent dévitalisation petites juridictions accès plus restreint juge réforme justice pourquoi avocats magistrats greffe colère dauphiné libérédepuis 1958 tribunaux instance ti tribunaux grande instance tgi partageaient contentieux civils selon répartition essentiellement fondée montant litige héritiers juges paix juges instance surnommés juges pauvres tranchaient toutes affaires lesquelles demande portait inférieures 10 000 euros expulsions locatives dettes impayées passant travaux mal exécutés conflits liés accidents circulation également compétents tutelles 1er janvier 285 tribunaux instance disparaissent ainsi 164 tgi france recours accru procédures dématérialisées quand tribunal instance situé commune tgi 57 ti concernés cette situation fusionnent former tribunal judiciaire quand ti situé commune différente comme ivry seine val marne condom gers molsheim bas rhin devient chambre détachée tribunal judiciaire appelé tribunal proximité alors particuliers pouvaient présenter directement greffe tribunal instance déposer requête réforme renforce recours accru procédures dématérialisées étend représentation obligatoire avocat exit aussi juge instance appellera désormais juge contentieux protection restera magistrat spécialisé affaires liées vulnérabilités économiques sociales garde sceaux dû renoncer supprimer cette fonction statutaire devant bronca opposants réforme compétences ajoutés décret quid leurs compétences deux principaux syndicats magistrats dénoncent flou autour question loi facilite création pôles spécialisés départements plusieurs tribunaux grande instance permet attribuer compétences supplémentaires tribunaux proximité mieux adapter besoins particuliers territoires souligne ministère justice ajouts compétences spécialisations décidés décrets après propositions chefs cours appel sans calendrier précis relèvent union syndicale magistrats usm syndicat magistrature sm plus gros bouleversement beaucoup cabinets juges instruction vont être supprimés juges application peines sait a aucune visibilité tacle présidente usm céline parisot calculs électoraux collègues savent chefs cour proposé ministre abonde katia dubreuil présidente sm déplorant absence concertation étonnée gouvernement souhaité différer annonces selon résultats électoraux république marche communes concernées comme écrivait canard enchaîné série articles fin octobre empêtrée cette polémique ministre bornée défendre toute partialité déplorant fusion conduite aveugle précipitation syndicats magistrats voient volonté faire économies échelle mutualisant effectifs greffes tribunaux aussi tribunaux conseils prud hommes malgré insistance garde sceaux répéter tous sites maintenus syndicats redoutent cette réforme prélude refonte carte judiciaire où tribunaux proximité vidés substance finiraient fermer depuis annonce cette fusion a près deux ans garde sceaux nicole belloubet invoque nécessité simplification lisibilité porte entrée unique justice\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the cleaned summaries and texts to ensure they have been cleaned well\n",
    "for i in range(3):\n",
    "    print(\"Clean News #\",i+1)\n",
    "    print(clean_target[i])\n",
    "    print(clean_input[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ba648702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(count_dict, text):\n",
    "    '''Count the number of occurrences of each word in a set of text'''\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ec3267a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 111529\n"
     ]
    }
   ],
   "source": [
    "# Find the number of times each word was used and the size of the vocabulary\n",
    "word_counts = {}\n",
    "\n",
    "count_words(word_counts, clean_target)\n",
    "count_words(word_counts, clean_input)\n",
    "            \n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3593256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import FastText, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "797aa0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load of pretrained embeddings vectors :\n",
    "pretrained_vectors_fasttext = FastText(language='fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "26e494cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pre-trained vocabulary contains 1152449 words, with embeddings in vectors of size 300\n"
     ]
    }
   ],
   "source": [
    "print(f'The pre-trained vocabulary contains {pretrained_vectors_fasttext.vectors.shape[0]} words, with embeddings in vectors of size {pretrained_vectors_fasttext.vectors.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "04f968c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_vectors_words = pretrained_vectors_fasttext.stoi.keys()\n",
    "pretrained_vectors_values = pretrained_vectors_fasttext.stoi.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d1b15367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1663, -0.2434, -0.1298,  0.2558,  0.2620,  0.4340,  0.2389,  0.2846,\n",
       "        -0.0285,  0.2952, -0.1806, -0.0203,  0.2095,  0.2392,  0.4044,  0.2178,\n",
       "         0.3261,  0.1015,  0.1417, -0.1413, -0.1626, -0.6919, -0.1303,  0.5766,\n",
       "         0.2136, -0.0434, -0.4864,  0.2376, -0.3875,  0.0248,  0.5002,  0.4109,\n",
       "        -0.2349,  0.2109, -0.1231, -0.1220, -0.2864, -0.2508, -0.2469,  0.0470,\n",
       "         0.2941, -0.2932, -0.0470, -0.0928,  0.0722, -0.0158,  0.2090,  0.1393,\n",
       "         0.3059,  0.3177, -0.1812, -0.0239, -0.1266,  0.0802, -0.1903, -0.2608,\n",
       "        -0.3757, -0.0703,  0.3611,  0.2268, -0.1355,  0.2499, -0.0559, -0.1626,\n",
       "         0.1937,  0.3333, -0.0398, -0.0106, -0.2556, -0.2036,  0.3537,  0.0297,\n",
       "         0.0255, -0.1837,  0.1164, -0.3757,  0.2895, -0.2726,  0.0061, -0.2071,\n",
       "        -0.2901, -0.0297, -0.0647,  0.1851, -0.0209, -0.0855,  0.0574,  0.3292,\n",
       "        -0.3409, -0.4960, -0.1257, -0.3342,  0.0513, -0.0179,  0.0588,  0.0645,\n",
       "        -0.2976,  0.0638,  0.2410, -0.2550, -0.2986, -0.2085, -0.2570,  0.4697,\n",
       "         0.0947,  0.3796,  0.2746, -0.4441,  0.2341,  0.0768, -0.0301, -0.0469,\n",
       "         0.1435,  0.2824,  0.2580, -0.2594,  0.2596, -0.0618, -0.0595, -0.1947,\n",
       "         0.3589, -0.1353, -0.4342, -0.2739, -0.0385,  0.1818,  0.1305, -0.1849,\n",
       "         0.1211, -0.0591, -0.1312,  0.1912,  0.3404, -0.0382,  0.1397, -0.0051,\n",
       "        -0.2034,  0.0856, -0.0016,  0.0147,  0.0628,  0.1353, -0.2379,  0.1144,\n",
       "         0.3324, -0.2130, -0.1240,  0.2746,  0.5219,  0.4409, -0.2259,  0.0107,\n",
       "        -0.0836,  0.2050,  0.1743, -0.0184,  0.1690,  0.1539,  0.0077,  0.4725,\n",
       "        -0.3211, -0.0922, -0.1920, -0.0969, -0.3629,  0.2679,  0.2557, -0.0857,\n",
       "        -0.1151, -0.1501, -0.1330,  0.0605,  0.0886, -0.0343, -0.0945, -0.1892,\n",
       "         0.0479, -0.1171,  0.2026, -0.1340, -0.0248,  0.1926,  0.5280, -0.2757,\n",
       "         0.3417, -0.0139,  0.0388, -0.1377, -0.2876, -0.5761, -0.1273, -0.2940,\n",
       "         0.1769,  0.2692,  0.1415, -0.1625,  0.0234,  0.0058, -0.5362, -0.1814,\n",
       "         0.6860, -0.0921,  0.2931,  0.0845, -0.5805, -0.1804, -0.0793,  0.2759,\n",
       "        -0.1169,  0.6628,  0.1200,  0.2342, -0.0388, -0.1099,  0.0519,  0.1616,\n",
       "        -0.0277, -0.2439,  0.0415,  0.1379,  0.0974, -0.1609, -0.0360, -0.1602,\n",
       "         0.1139, -0.1470, -0.0983,  0.1412,  0.2697, -0.1490,  0.1723, -0.2127,\n",
       "         0.1073,  0.3893, -0.1609,  0.1174,  0.4289, -0.1584,  0.1251,  0.4409,\n",
       "        -0.1356,  0.0074, -0.5189, -0.0558,  0.0380, -0.1450, -0.0888, -0.2765,\n",
       "         0.3083,  0.1420, -0.0556,  0.3881, -0.0875,  0.0239, -0.1238, -0.0334,\n",
       "        -0.1900, -0.2796, -0.1100, -0.1270, -0.0018, -0.0813,  0.0889, -0.1201,\n",
       "        -0.0937, -0.0096, -0.1183, -0.1321, -0.0857,  0.0891,  0.2931, -0.0423,\n",
       "         0.2086,  0.5333, -0.2142, -0.2307, -0.3187, -0.0775, -0.1164, -0.2546,\n",
       "         0.0318, -0.0810,  0.1116,  0.1264, -0.1230, -0.2088,  0.0108,  0.4909,\n",
       "         0.1728, -0.1331, -0.0080, -0.3447,  0.5024, -0.0092, -0.0625, -0.1091,\n",
       "        -0.1239, -0.1336,  0.2848, -0.1123])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_vectors_fasttext['maison']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2f07deac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 111529\n"
     ]
    }
   ],
   "source": [
    "# Load Conceptnet Numberbatch's (CN) embeddings, similar to GloVe, but probably better \n",
    "# (https://github.com/commonsense/conceptnet-numberbatch)\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import timeit\n",
    "\n",
    "\n",
    "#data_path = 'drive/MyDrive/numberbatch-fr.txt'\n",
    "#data_path = 'drive/MyDrive/Colab Notebooks/numberbatch-fr-clean.txt'\n",
    "#data_path = 'C:/Users/Giuseppe/Desktop/NLP/Embeddings/cc.fr.300.vec.gz'\n",
    "\n",
    "embeddings_index = {}#'rb' encoding='utf-8'\n",
    "\n",
    "#word_dict = []\n",
    "#with open(data_path, 'r', encoding='utf-8') as f: \n",
    " #   for line in f:\n",
    "        #values = line.split(' ')\n",
    "        #line = re.split(r'fr/',line)\n",
    "        #values = re.split(\" \", line[1])\n",
    "word = pretrained_vectors_fasttext.stoi.keys()#[12:]\n",
    "        #values = pretrained_vectors_fasttext.stoi.values()\n",
    "        #word_dict.append(word) \n",
    "for word in word_counts:\n",
    "    embedding = pretrained_vectors_fasttext[word]\n",
    "            #embedding = np.asarray(values[1:])#, dtype='float32'\n",
    "    embeddings_index[word] = embedding\n",
    "        #print(word)\n",
    "print('Word embeddings:', len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6958bc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words missing from fastText: 0\n",
      "Percent of words that are missing from vocabulary: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Find the number of words that are missing from CN, and are used more than our threshold.\n",
    "missing_words = 0\n",
    "threshold = 20\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    if count > threshold:\n",
    "        if word not in embeddings_index:\n",
    "            missing_words += 1\n",
    "            \n",
    "missing_ratio = round(missing_words/len(word_counts),4)*100\n",
    "            \n",
    "print(\"Number of words missing from fastText:\", missing_words)\n",
    "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9d224daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 111529\n",
      "Number of words we will use: 111533\n",
      "Percent of words we will use: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Limit the vocab that we will use to words that appear ≥ threshold or are in CN\n",
    "\n",
    "#dictionary to convert words to integers\n",
    "vocab_to_int = {} \n",
    "\n",
    "value = 0\n",
    "for word, count in word_counts.items():\n",
    "    if count >= threshold or word in embeddings_index:\n",
    "        vocab_to_int[word] = value\n",
    "        value += 1\n",
    "\n",
    "# Special tokens that will be added to our vocab\n",
    "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
    "\n",
    "# Add codes to vocab\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = len(vocab_to_int)\n",
    "\n",
    "# Dictionary to convert integers to words\n",
    "int_to_vocab = {}\n",
    "for word, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = word\n",
    "\n",
    "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
    "\n",
    "print(\"Total number of unique words:\", len(word_counts))\n",
    "print(\"Number of words we will use:\", len(vocab_to_int))\n",
    "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fa95ee68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111533\n"
     ]
    }
   ],
   "source": [
    "# Need to use 300 for embedding dimensions to match CN's vectors.\n",
    "embedding_dim = 300\n",
    "nb_words = len(vocab_to_int)\n",
    "\n",
    "# Create matrix with default values of zero\n",
    "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "for word, i in vocab_to_int.items():\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        # If word not in CN, create a random embedding for it\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        embeddings_index[word] = new_embedding\n",
    "        word_embedding_matrix[i] = new_embedding\n",
    "\n",
    "# Check if value matches len(vocab_to_int)\n",
    "print(len(word_embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "712aba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
    "    '''Convert words in text to an integer.\n",
    "       If word is not in vocab_to_int, use UNK's integer.\n",
    "       Total the number of words and UNKs.\n",
    "       Add EOS token to the end of texts'''\n",
    "    ints = []\n",
    "    for sentence in text:\n",
    "        sentence_ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                sentence_ints.append(vocab_to_int[word])\n",
    "            else:\n",
    "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
    "                unk_count += 1\n",
    "        if eos:\n",
    "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
    "        ints.append(sentence_ints)\n",
    "    return ints, word_count, unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2aaa153a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in headlines: 5339165\n",
      "Total number of UNKs in headlines: 0\n",
      "Percent of words that are UNK: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Apply convert_to_ints to clean_target and clean_input\n",
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "int_target, word_count, unk_count = convert_to_ints(clean_target, word_count, unk_count)\n",
    "int_input, word_count, unk_count = convert_to_ints(clean_input, word_count, unk_count, eos=True)\n",
    "\n",
    "unk_percent = round(unk_count/word_count,4)*100\n",
    "\n",
    "print(\"Total number of words in headlines:\", word_count)\n",
    "print(\"Total number of UNKs in headlines:\", unk_count)\n",
    "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c481b2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lengths(text):\n",
    "    '''Create a data frame of the sentence lengths from a text'''\n",
    "    lengths = []\n",
    "    for sentence in text:\n",
    "        lengths.append(len(sentence))\n",
    "    return pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "640d4989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries:\n",
      "             counts\n",
      "count  21401.000000\n",
      "mean      34.393112\n",
      "std       12.316238\n",
      "min        3.000000\n",
      "25%       26.000000\n",
      "50%       34.000000\n",
      "75%       42.000000\n",
      "max      164.000000\n",
      "\n",
      "Texts:\n",
      "             counts\n",
      "count  21401.000000\n",
      "mean     216.088921\n",
      "std      106.650884\n",
      "min       11.000000\n",
      "25%      142.000000\n",
      "50%      192.000000\n",
      "75%      260.000000\n",
      "max     1884.000000\n"
     ]
    }
   ],
   "source": [
    "lengths_target = create_lengths(int_target)\n",
    "lengths_input = create_lengths(int_input)\n",
    "\n",
    "print(\"Summaries:\")\n",
    "print(lengths_target.describe())\n",
    "print()\n",
    "print(\"Texts:\")\n",
    "print(lengths_input.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c1c42992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "377.0\n",
      "417.0\n",
      "525.0\n"
     ]
    }
   ],
   "source": [
    "# Inspect the length of \"input\"\n",
    "print(np.percentile(lengths_input.counts, 90))\n",
    "print(np.percentile(lengths_input.counts, 95))\n",
    "print(np.percentile(lengths_input.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "943c8732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.0\n",
      "54.0\n",
      "70.0\n"
     ]
    }
   ],
   "source": [
    "# Inspect the length of \"target\"\n",
    "print(np.percentile(lengths_target.counts, 90))\n",
    "print(np.percentile(lengths_target.counts, 95))\n",
    "print(np.percentile(lengths_target.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "449361b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unk_counter(sentence):\n",
    "    '''Counts the number of time UNK appears in a sentence.'''\n",
    "    unk_count = 0\n",
    "    for word in sentence:\n",
    "        if word == vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "    return unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "37e0a745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19058\n",
      "19058\n"
     ]
    }
   ],
   "source": [
    "# takes a long time  , this is normal\n",
    "\n",
    "# Sort the summaries and texts by the length of the texts, shortest to longest\n",
    "# Limit the length of summaries and texts based on the min and max ranges.\n",
    "# Remove texts that include too many UNKs\n",
    "\n",
    "sorted_target = []\n",
    "sorted_input = []\n",
    "max_input_length = 377\n",
    "max_target_length = 70\n",
    "min_length = 2\n",
    "unk_input_limit = 1\n",
    "unk_target_limit = 0\n",
    "\n",
    "for length in range(min(lengths_input.counts), max_input_length): \n",
    "    for count, words in enumerate(int_target):\n",
    "        if (len(int_target[count]) >= min_length and\n",
    "            len(int_target[count]) <= max_target_length and\n",
    "            len(int_input[count]) >= min_length and\n",
    "            unk_counter(int_target[count]) <= unk_target_limit and\n",
    "            unk_counter(int_input[count]) <= unk_input_limit and\n",
    "            length == len(int_input[count])\n",
    "           ):\n",
    "            sorted_target.append(int_target[count])\n",
    "            sorted_input.append(int_input[count])\n",
    "        \n",
    "# Compare lengths to ensure they match\n",
    "print(len(sorted_target))\n",
    "print(len(sorted_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd0dda7",
   "metadata": {},
   "source": [
    "Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "4379f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    '''Create placeholders for inputs to the model'''\n",
    "    \n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
    "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
    "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
    "\n",
    "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "bbe89dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
    "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
    "    \n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7a9c7ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
    "    '''Create the encoding layer'''\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
    "                                                                    cell_bw, \n",
    "                                                                    rnn_inputs,\n",
    "                                                                    sequence_length,\n",
    "                                                                    dtype=tf.float32)\n",
    "    # Join outputs since we are using a bidirectional RNN\n",
    "    enc_output = tf.concat(enc_output,2)\n",
    "    \n",
    "    return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d508a4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state, output_layer, \n",
    "                            vocab_size, max_summary_length):\n",
    "    '''Create the training logits'''\n",
    "    \n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                        sequence_length=summary_length,\n",
    "                                                        time_major=False)\n",
    "\n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                       training_helper,\n",
    "                                                       initial_state,\n",
    "                                                       output_layer) \n",
    "\n",
    "    training_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                           output_time_major=False,\n",
    "                                                           impute_finished=True,\n",
    "                                                           maximum_iterations=max_summary_length)\n",
    "    return training_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e73defe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
    "                             max_summary_length, batch_size):\n",
    "    '''Create the inference logits'''\n",
    "    \n",
    "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "    \n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                                start_tokens,\n",
    "                                                                end_token)\n",
    "                \n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                        inference_helper,\n",
    "                                                        initial_state,\n",
    "                                                        output_layer)\n",
    "                \n",
    "    inference_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                            output_time_major=False,\n",
    "                                                            impute_finished=True,\n",
    "                                                            maximum_iterations=max_summary_length)\n",
    "    \n",
    "    return inference_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2136bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length, \n",
    "                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
    "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('decoder_{}'.format(layer)):\n",
    "            lstm = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n",
    "                                                     input_keep_prob = keep_prob)\n",
    "    \n",
    "    output_layer = Dense(vocab_size,\n",
    "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    \n",
    "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
    "                                                  enc_output,\n",
    "                                                  text_length,\n",
    "                                                  normalize=False,\n",
    "                                                  name='BahdanauAttention')\n",
    "\n",
    "    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,\n",
    "                                                          attn_mech,\n",
    "                                                          rnn_size)\n",
    "            \n",
    "    #initial_state = tf.contrib.seq2seq.AttentionWrapperState(enc_state[0],\n",
    "    #                                                                _zero_state_tensors(rnn_size, \n",
    "    #                                                                                    batch_size, \n",
    "    #                                                                                    tf.float32)) \n",
    "    initial_state = dec_cell.zero_state(batch_size=batch_size,dtype=tf.float32).clone(cell_state=enc_state[0])\n",
    "\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        training_decoder = training_decoding_layer(dec_embed_input, \n",
    "                                                  summary_length, \n",
    "                                                  dec_cell, \n",
    "                                                  initial_state,\n",
    "                                                  output_layer,\n",
    "                                                  vocab_size, \n",
    "                                                  max_summary_length)\n",
    "        \n",
    "        training_logits,_ ,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                  output_time_major=False,\n",
    "                                  impute_finished=True,\n",
    "                                  maximum_iterations=max_summary_length)\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        inference_decoder = inference_decoding_layer(embeddings,  \n",
    "                                                    vocab_to_int['<GO>'], \n",
    "                                                    vocab_to_int['<EOS>'],\n",
    "                                                    dec_cell, \n",
    "                                                    initial_state, \n",
    "                                                    output_layer,\n",
    "                                                    max_summary_length,\n",
    "                                                    batch_size)\n",
    "        \n",
    "        inference_logits,_ ,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                  output_time_major=False,\n",
    "                                  impute_finished=True,\n",
    "                                  maximum_iterations=max_summary_length)\n",
    "\n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e48adb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
    "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
    "    '''Use the previous functions to create the training and inference logits'''\n",
    "    \n",
    "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
    "    embeddings = word_embedding_matrix\n",
    "    \n",
    "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
    "    \n",
    "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size)\n",
    "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
    "    \n",
    "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
    "                                                        embeddings,\n",
    "                                                        enc_output,\n",
    "                                                        enc_state, \n",
    "                                                        vocab_size, \n",
    "                                                        text_length, \n",
    "                                                        summary_length, \n",
    "                                                        max_summary_length,\n",
    "                                                        rnn_size, \n",
    "                                                        vocab_to_int, \n",
    "                                                        keep_prob, \n",
    "                                                        batch_size,\n",
    "                                                        num_layers)\n",
    "    \n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c1e7ad4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e7665a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(summaries, texts, batch_size):\n",
    "    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(texts)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
    "        texts_batch = texts[start_i:start_i + batch_size]\n",
    "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
    "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_summaries_lengths = []\n",
    "        for summary in pad_summaries_batch:\n",
    "            pad_summaries_lengths.append(len(summary))\n",
    "        \n",
    "        pad_texts_lengths = []\n",
    "        for text in pad_texts_batch:\n",
    "            pad_texts_lengths.append(len(text))\n",
    "        \n",
    "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "85de84c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Hyperparameters\n",
    "epochs = 200\n",
    "batch_size = 64\n",
    "rnn_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.005\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "77e2d906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph is built.\n"
     ]
    }
   ],
   "source": [
    "# Build the graph\n",
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs    \n",
    "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
    "\n",
    "    # Create the training and inference logits\n",
    "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                      targets, \n",
    "                                                      keep_prob,   \n",
    "                                                      text_length,\n",
    "                                                      summary_length,\n",
    "                                                      max_summary_length,\n",
    "                                                      len(vocab_to_int)+1,\n",
    "                                                      rnn_size, \n",
    "                                                      num_layers, \n",
    "                                                      vocab_to_int,\n",
    "                                                      batch_size)\n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "    \n",
    "    # Create the weights for sequence_loss\n",
    "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "print(\"Graph is built.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01216f9d",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "2ac60ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shortest text length: 186\n",
      "The longest text length: 331\n"
     ]
    }
   ],
   "source": [
    "# Subset the data for training\n",
    "start = 10000\n",
    "end = start + 8000\n",
    "sorted_target_short = sorted_target[start:end]\n",
    "sorted_input_short = sorted_input[start:end]\n",
    "print(\"The shortest text length:\", len(sorted_input_short[0]))\n",
    "print(\"The longest text length:\",len(sorted_input_short[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "659999f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/200 Batch   20/125 - Loss:  5.555, Seconds: 693.84\n",
      "Epoch   1/200 Batch   40/125 - Loss:  4.108, Seconds: 627.71\n",
      "Average loss for this update: 4.831\n",
      "New Record!\n",
      "Epoch   1/200 Batch   60/125 - Loss:  4.030, Seconds: 721.46\n",
      "Epoch   1/200 Batch   80/125 - Loss:  3.918, Seconds: 576.46\n",
      "Average loss for this update: 3.974\n",
      "New Record!\n",
      "Epoch   1/200 Batch  100/125 - Loss:  3.822, Seconds: 712.07\n",
      "Epoch   1/200 Batch  120/125 - Loss:  3.818, Seconds: 774.20\n",
      "Average loss for this update: 3.82\n",
      "New Record!\n",
      "Epoch   2/200 Batch   20/125 - Loss:  3.599, Seconds: 719.46\n",
      "Epoch   2/200 Batch   40/125 - Loss:  3.334, Seconds: 676.93\n",
      "Average loss for this update: 3.467\n",
      "New Record!\n",
      "Epoch   2/200 Batch   60/125 - Loss:  3.291, Seconds: 722.74\n",
      "Epoch   2/200 Batch   80/125 - Loss:  3.229, Seconds: 573.00\n",
      "Average loss for this update: 3.26\n",
      "New Record!\n",
      "Epoch   2/200 Batch  100/125 - Loss:  3.191, Seconds: 689.84\n",
      "Epoch   2/200 Batch  120/125 - Loss:  3.232, Seconds: 785.48\n",
      "Average loss for this update: 3.211\n",
      "New Record!\n",
      "Epoch   3/200 Batch   20/125 - Loss:  3.146, Seconds: 711.87\n",
      "Epoch   3/200 Batch   40/125 - Loss:  2.950, Seconds: 671.35\n",
      "Average loss for this update: 3.048\n",
      "New Record!\n",
      "Epoch   3/200 Batch   60/125 - Loss:  2.938, Seconds: 722.97\n",
      "Epoch   3/200 Batch   80/125 - Loss:  2.886, Seconds: 603.12\n",
      "Average loss for this update: 2.912\n",
      "New Record!\n",
      "Epoch   3/200 Batch  100/125 - Loss:  2.858, Seconds: 756.14\n",
      "Epoch   3/200 Batch  120/125 - Loss:  2.927, Seconds: 778.42\n",
      "Average loss for this update: 2.892\n",
      "New Record!\n",
      "Epoch   4/200 Batch   20/125 - Loss:  2.882, Seconds: 688.16\n",
      "Epoch   4/200 Batch   40/125 - Loss:  2.708, Seconds: 674.63\n",
      "Average loss for this update: 2.795\n",
      "New Record!\n",
      "Epoch   4/200 Batch   60/125 - Loss:  2.693, Seconds: 729.54\n",
      "Epoch   4/200 Batch   80/125 - Loss:  2.672, Seconds: 598.80\n",
      "Average loss for this update: 2.682\n",
      "New Record!\n",
      "Epoch   4/200 Batch  100/125 - Loss:  2.651, Seconds: 745.67\n",
      "Epoch   4/200 Batch  120/125 - Loss:  2.728, Seconds: 789.96\n",
      "Average loss for this update: 2.689\n",
      "No Improvement.\n",
      "Epoch   5/200 Batch   20/125 - Loss:  2.697, Seconds: 699.02\n",
      "Epoch   5/200 Batch   40/125 - Loss:  2.540, Seconds: 724.81\n",
      "Average loss for this update: 2.618\n",
      "New Record!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19416\\2882495716.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m                  \u001b[0msummary_length\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msummaries_lengths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m                  \u001b[0mtext_length\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtexts_lengths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m                  keep_prob: keep_probability})\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mbatch_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "learning_rate_decay = 0.95\n",
    "min_learning_rate = 0.01#0.0005\n",
    "display_step = 20 # Check training loss after every 20 batches\n",
    "stop_early = 0 \n",
    "stop = 6 #3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
    "per_epoch = 3 # Make 3 update checks per epoch\n",
    "update_check = (len(sorted_input_short)//batch_size//per_epoch)-1\n",
    "\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
    "\n",
    "  \n",
    "tf.reset_default_graph()\n",
    "checkpoint = \"C:/Users/Giuseppe/Desktop/NLP/best_model.ckpt\"  #300k sentence\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # If we want to continue training a previous session\n",
    "    # loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    # loader.restore(sess, checkpoint)\n",
    "    #sess.run(tf.local_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        update_loss = 0\n",
    "        batch_loss = 0\n",
    "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
    "                get_batches(sorted_target_short, sorted_input_short, batch_size)):\n",
    "            start_time = time.time()\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: texts_batch,\n",
    "                 targets: summaries_batch,\n",
    "                 lr: learning_rate,\n",
    "                 summary_length: summaries_lengths,\n",
    "                 text_length: texts_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "            batch_loss += loss\n",
    "            update_loss += loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(sorted_input_short) // batch_size, \n",
    "                              batch_loss / display_step, \n",
    "                              batch_time*display_step))\n",
    "                batch_loss = 0\n",
    "                \n",
    "                saver = tf.train.Saver() \n",
    "                saver.save(sess, checkpoint)\n",
    "                \n",
    "            if batch_i % update_check == 0 and batch_i > 0:\n",
    "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                summary_update_loss.append(update_loss)\n",
    "                \n",
    "              \n",
    "                  \n",
    "                # If the update loss is at a new minimum, save the model\n",
    "                if update_loss <= min(summary_update_loss):\n",
    "                    print('New Record!') \n",
    "                    stop_early = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    saver.save(sess, checkpoint)\n",
    "\n",
    "                else:\n",
    "                    print(\"No Improvement.\")\n",
    "                    stop_early += 1\n",
    "                    if stop_early == stop:\n",
    "                        break\n",
    "                update_loss = 0\n",
    "            \n",
    "                    \n",
    "        # Reduce learning rate, but not below its minimum value\n",
    "        learning_rate *= learning_rate_decay\n",
    "        if learning_rate < min_learning_rate:\n",
    "            learning_rate = min_learning_rate\n",
    "        \n",
    "        if stop_early == stop:\n",
    "            print(\"Stopping Training.\")\n",
    "            break\n",
    "    saver.save(sess, checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8d4abf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:/Users/Giuseppe/Desktop/NLP/best_model.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['input',\n",
       " 'targets',\n",
       " 'learning_rate',\n",
       " 'keep_prob',\n",
       " 'summary_length',\n",
       " 'Const',\n",
       " 'max_dec_len',\n",
       " 'text_length',\n",
       " 'ReverseV2/axis',\n",
       " 'ReverseV2',\n",
       " 'embedding_lookup/params_0',\n",
       " 'embedding_lookup/axis',\n",
       " 'embedding_lookup',\n",
       " 'embedding_lookup/Identity',\n",
       " 'encoder_0/DropoutWrapperInit/Const',\n",
       " 'encoder_0/DropoutWrapperInit/Const_1',\n",
       " 'encoder_0/DropoutWrapperInit_1/Const',\n",
       " 'encoder_0/DropoutWrapperInit_1/Const_1',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/Rank',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/range/start',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/range/delta',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/range',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/concat/values_0',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/concat/axis',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/concat',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/transpose',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/sequence_length',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/Shape',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice/stack',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice/stack_1',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice/stack_2',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims/dim',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat/axis',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros/Const',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1/dim',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const_1',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2/dim',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const_2',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1/axis',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1/Const',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3/dim',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const_3',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/Shape_1',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/stack',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/Equal',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/Const',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/All',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/Assert/Const',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/Assert/Const_1',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/Assert/Assert/data_0',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/Assert/Assert/data_2',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/Assert/Assert',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/CheckSeqLen',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/Shape_2',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_1/stack',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_1/stack_1',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_1/stack_2',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_1',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/Shape_3',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_2/stack',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_2/stack_1',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_2/stack_2',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_2',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/ExpandDims/dim',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/ExpandDims',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/Const_1',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/concat_1/axis',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/concat_1',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/zeros/Const',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/zeros',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/Const_2',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/Min',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/Const_3',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/Max',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/time',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/TensorArray',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/TensorArray_1',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/Shape',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice/stack',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice/stack_1',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice/stack_2',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/range/start',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/range/delta',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/range',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/Maximum/x',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/Maximum',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/Minimum',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/iteration_counter',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/Enter',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/Enter_1',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/Enter_2',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/Enter_3',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/Enter_4',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/Merge',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/Merge_1',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/Merge_2',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/Merge_3',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/Merge_4',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/Less/Enter',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/Less',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/Less_1/Enter',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/Less_1',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/LogicalAnd',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/LoopCond',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/Switch',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/Switch_1',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/Switch_2',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/Switch_3',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/Switch_4',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/Identity',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/Identity_1',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/Identity_2',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/Identity_3',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/Identity_4',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/add/y',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/add',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Enter',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Enter_1',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/TensorArrayReadV3',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/GreaterEqual/Enter',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/GreaterEqual',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/sub/x',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/sub/Enter',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/sub',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/Shape',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/sub/x',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/sub',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/random_uniform/min',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/random_uniform/max',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/random_uniform/RandomUniform',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/random_uniform/sub',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/random_uniform/mul',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/random_uniform',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/add',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/Floor',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/truediv',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/mul',\n",
       " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/shape',\n",
       " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/min',\n",
       " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/max',\n",
       " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/RandomUniform',\n",
       " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/sub',\n",
       " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/mul',\n",
       " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform',\n",
       " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel',\n",
       " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/IsInitialized/VarIsInitializedOp',\n",
       " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Assign',\n",
       " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Read/ReadVariableOp',\n",
       " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Read/Identity',\n",
       " 'encoder_0/bidirectional_rnn/fw/lstm_cell/bias/Initializer/zeros/shape_as_tensor',\n",
       " 'encoder_0/bidirectional_rnn/fw/lstm_cell/bias/Initializer/zeros/Const',\n",
       " 'encoder_0/bidirectional_rnn/fw/lstm_cell/bias/Initializer/zeros',\n",
       " 'encoder_0/bidirectional_rnn/fw/lstm_cell/bias',\n",
       " 'encoder_0/bidirectional_rnn/fw/lstm_cell/bias/IsInitialized/VarIsInitializedOp',\n",
       " 'encoder_0/bidirectional_rnn/fw/lstm_cell/bias/Assign',\n",
       " 'encoder_0/bidirectional_rnn/fw/lstm_cell/bias/Read/ReadVariableOp',\n",
       " 'encoder_0/bidirectional_rnn/fw/lstm_cell/bias/Read/Identity',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat/axis',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/MatMul/Enter',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/MatMul',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd/Enter',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/Const',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/split/split_dim',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/split',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/add/y',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/add',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/Sigmoid',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/mul',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/Sigmoid_1',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/Tanh',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/mul_1',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/add_1',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/Sigmoid_2',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/Tanh_1',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/mul_2',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/Select/Enter',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/Select',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/Select_1',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/Select_2',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3/Enter',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/add_1/y',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/add_1',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/NextIteration',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/NextIteration_1',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/NextIteration_2',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/NextIteration_3',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/NextIteration_4',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/Exit',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/Exit_1',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/Exit_2',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/Exit_3',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/while/Exit_4',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayStack/TensorArraySizeV3',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayStack/range/start',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayStack/range/delta',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayStack/range',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayStack/TensorArrayGatherV3',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/Const_4',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/Rank_1',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/range_1/start',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/range_1/delta',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/range_1',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/concat_2/values_0',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/concat_2/axis',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/concat_2',\n",
       " 'encoder_0/bidirectional_rnn/fw/fw/transpose_1',\n",
       " 'encoder_0/bidirectional_rnn/bw/ReverseSequence',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/Rank',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/range/start',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/range/delta',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/range',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/concat/values_0',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/concat/axis',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/concat',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/transpose',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/sequence_length',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/Shape',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice/stack',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice/stack_1',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice/stack_2',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims/dim',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat/axis',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros/Const',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1/dim',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const_1',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2/dim',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const_2',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1/axis',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1/Const',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3/dim',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const_3',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/Shape_1',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/stack',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/Equal',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/Const',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/All',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/Assert/Const',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/Assert/Const_1',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/Assert/Assert/data_0',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/Assert/Assert/data_2',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/Assert/Assert',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/CheckSeqLen',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/Shape_2',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_1/stack',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_1/stack_1',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_1/stack_2',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_1',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/Shape_3',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_2/stack',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_2/stack_1',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_2/stack_2',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_2',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/ExpandDims/dim',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/ExpandDims',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/Const_1',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/concat_1/axis',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/concat_1',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/zeros/Const',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/zeros',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/Const_2',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/Min',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/Const_3',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/Max',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/time',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/TensorArray',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/TensorArray_1',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/Shape',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice/stack',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice/stack_1',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice/stack_2',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/range/start',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/range/delta',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/range',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/Maximum/x',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/Maximum',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/Minimum',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/iteration_counter',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/Enter',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/Enter_1',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/Enter_2',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/Enter_3',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/Enter_4',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/Merge',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/Merge_1',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/Merge_2',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/Merge_3',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/Merge_4',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/Less/Enter',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/Less',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/Less_1/Enter',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/Less_1',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/LogicalAnd',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/LoopCond',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/Switch',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/Switch_1',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/Switch_2',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/Switch_3',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/Switch_4',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/Identity',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/Identity_1',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/Identity_2',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/Identity_3',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/Identity_4',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/add/y',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/add',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/TensorArrayReadV3/Enter',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/TensorArrayReadV3/Enter_1',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/TensorArrayReadV3',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/GreaterEqual/Enter',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/GreaterEqual',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/sub/x',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/sub/Enter',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/sub',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/Shape',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/sub/x',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/sub',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/random_uniform/min',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/random_uniform/max',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/random_uniform/RandomUniform',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/random_uniform/sub',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/random_uniform/mul',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/random_uniform',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/add',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/Floor',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/truediv',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/mul',\n",
       " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/shape',\n",
       " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/min',\n",
       " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/max',\n",
       " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/RandomUniform',\n",
       " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/sub',\n",
       " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/mul',\n",
       " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform',\n",
       " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel',\n",
       " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/IsInitialized/VarIsInitializedOp',\n",
       " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Assign',\n",
       " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Read/ReadVariableOp',\n",
       " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Read/Identity',\n",
       " 'encoder_0/bidirectional_rnn/bw/lstm_cell/bias/Initializer/zeros/shape_as_tensor',\n",
       " 'encoder_0/bidirectional_rnn/bw/lstm_cell/bias/Initializer/zeros/Const',\n",
       " 'encoder_0/bidirectional_rnn/bw/lstm_cell/bias/Initializer/zeros',\n",
       " 'encoder_0/bidirectional_rnn/bw/lstm_cell/bias',\n",
       " 'encoder_0/bidirectional_rnn/bw/lstm_cell/bias/IsInitialized/VarIsInitializedOp',\n",
       " 'encoder_0/bidirectional_rnn/bw/lstm_cell/bias/Assign',\n",
       " 'encoder_0/bidirectional_rnn/bw/lstm_cell/bias/Read/ReadVariableOp',\n",
       " 'encoder_0/bidirectional_rnn/bw/lstm_cell/bias/Read/Identity',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat/axis',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/MatMul/Enter',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/MatMul',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd/Enter',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/Const',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/split/split_dim',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/split',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/add/y',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/add',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/Sigmoid',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/mul',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/Sigmoid_1',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/Tanh',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/mul_1',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/add_1',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/Sigmoid_2',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/Tanh_1',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/mul_2',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/Select/Enter',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/Select',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/Select_1',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/Select_2',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/TensorArrayWrite/TensorArrayWriteV3/Enter',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/TensorArrayWrite/TensorArrayWriteV3',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/add_1/y',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/add_1',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/NextIteration',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/NextIteration_1',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/NextIteration_2',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/NextIteration_3',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/NextIteration_4',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/Exit',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/Exit_1',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/Exit_2',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/Exit_3',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/while/Exit_4',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayStack/TensorArraySizeV3',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayStack/range/start',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayStack/range/delta',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayStack/range',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayStack/TensorArrayGatherV3',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/Const_4',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/Rank_1',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/range_1/start',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/range_1/delta',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/range_1',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/concat_2/values_0',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/concat_2/axis',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/concat_2',\n",
       " 'encoder_0/bidirectional_rnn/bw/bw/transpose_1',\n",
       " 'encoder_0/ReverseSequence',\n",
       " 'encoder_1/DropoutWrapperInit/Const',\n",
       " 'encoder_1/DropoutWrapperInit/Const_1',\n",
       " 'encoder_1/DropoutWrapperInit_1/Const',\n",
       " 'encoder_1/DropoutWrapperInit_1/Const_1',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/Rank',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/range/start',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/range/delta',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/range',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/concat/values_0',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/concat/axis',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/concat',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/transpose',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/sequence_length',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/Shape',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice/stack',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice/stack_1',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice/stack_2',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims/dim',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat/axis',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros/Const',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1/dim',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const_1',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2/dim',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const_2',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1/axis',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1/Const',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3/dim',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const_3',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/Shape_1',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/stack',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/Equal',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/Const',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/All',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/Assert/Const',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/Assert/Const_1',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/Assert/Assert/data_0',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/Assert/Assert/data_2',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/Assert/Assert',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/CheckSeqLen',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/Shape_2',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_1/stack',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_1/stack_1',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_1/stack_2',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_1',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/Shape_3',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_2/stack',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_2/stack_1',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_2/stack_2',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_2',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/ExpandDims/dim',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/ExpandDims',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/Const_1',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/concat_1/axis',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/concat_1',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/zeros/Const',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/zeros',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/Const_2',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/Min',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/Const_3',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/Max',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/time',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/TensorArray',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/TensorArray_1',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/Shape',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice/stack',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice/stack_1',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice/stack_2',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/range/start',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/range/delta',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/range',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/Maximum/x',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/Maximum',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/Minimum',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/iteration_counter',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/Enter',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/Enter_1',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/Enter_2',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/Enter_3',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/Enter_4',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/Merge',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/Merge_1',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/Merge_2',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/Merge_3',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/Merge_4',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/Less/Enter',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/Less',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/Less_1/Enter',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/Less_1',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/LogicalAnd',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/LoopCond',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/Switch',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/Switch_1',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/Switch_2',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/Switch_3',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/Switch_4',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/Identity',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/Identity_1',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/Identity_2',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/Identity_3',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/Identity_4',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/add/y',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/add',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Enter',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Enter_1',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/TensorArrayReadV3',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/GreaterEqual/Enter',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/GreaterEqual',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/sub/x',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/sub/Enter',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/sub',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/Shape',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/sub/x',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/sub',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/random_uniform/min',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/random_uniform/max',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/random_uniform/RandomUniform',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/random_uniform/sub',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/random_uniform/mul',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/random_uniform',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/add',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/Floor',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/truediv',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/mul',\n",
       " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/shape',\n",
       " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/min',\n",
       " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/max',\n",
       " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/RandomUniform',\n",
       " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/sub',\n",
       " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/mul',\n",
       " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform',\n",
       " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel',\n",
       " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/IsInitialized/VarIsInitializedOp',\n",
       " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Assign',\n",
       " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Read/ReadVariableOp',\n",
       " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Read/Identity',\n",
       " 'encoder_1/bidirectional_rnn/fw/lstm_cell/bias/Initializer/zeros/shape_as_tensor',\n",
       " 'encoder_1/bidirectional_rnn/fw/lstm_cell/bias/Initializer/zeros/Const',\n",
       " 'encoder_1/bidirectional_rnn/fw/lstm_cell/bias/Initializer/zeros',\n",
       " 'encoder_1/bidirectional_rnn/fw/lstm_cell/bias',\n",
       " 'encoder_1/bidirectional_rnn/fw/lstm_cell/bias/IsInitialized/VarIsInitializedOp',\n",
       " 'encoder_1/bidirectional_rnn/fw/lstm_cell/bias/Assign',\n",
       " 'encoder_1/bidirectional_rnn/fw/lstm_cell/bias/Read/ReadVariableOp',\n",
       " 'encoder_1/bidirectional_rnn/fw/lstm_cell/bias/Read/Identity',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat/axis',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/MatMul/Enter',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/MatMul',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd/Enter',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/Const',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/split/split_dim',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/split',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/add/y',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/add',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/Sigmoid',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/mul',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/Sigmoid_1',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/Tanh',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/mul_1',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/add_1',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/Sigmoid_2',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/Tanh_1',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/mul_2',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/Select/Enter',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/Select',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/Select_1',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/Select_2',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3/Enter',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/add_1/y',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/add_1',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/NextIteration',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/NextIteration_1',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/NextIteration_2',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/NextIteration_3',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/NextIteration_4',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_1',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_2',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_4',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayStack/TensorArraySizeV3',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayStack/range/start',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayStack/range/delta',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayStack/range',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayStack/TensorArrayGatherV3',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/Const_4',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/Rank_1',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/range_1/start',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/range_1/delta',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/range_1',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/concat_2/values_0',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/concat_2/axis',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/concat_2',\n",
       " 'encoder_1/bidirectional_rnn/fw/fw/transpose_1',\n",
       " 'encoder_1/bidirectional_rnn/bw/ReverseSequence',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/Rank',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/range/start',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/range/delta',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/range',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/concat/values_0',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/concat/axis',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/concat',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/transpose',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/sequence_length',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/Shape',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice/stack',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice/stack_1',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice/stack_2',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims/dim',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat/axis',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros/Const',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1/dim',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const_1',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2/dim',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const_2',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1/axis',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1/Const',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3/dim',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const_3',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/Shape_1',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/stack',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/Equal',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/Const',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/All',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/Assert/Const',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/Assert/Const_1',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/Assert/Assert/data_0',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/Assert/Assert/data_2',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/Assert/Assert',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/CheckSeqLen',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/Shape_2',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_1/stack',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_1/stack_1',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_1/stack_2',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_1',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/Shape_3',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_2/stack',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_2/stack_1',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_2/stack_2',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_2',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/ExpandDims/dim',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/ExpandDims',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/Const_1',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/concat_1/axis',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/concat_1',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/zeros/Const',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/zeros',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/Const_2',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/Min',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/Const_3',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/Max',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/time',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/TensorArray',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/TensorArray_1',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/Shape',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice/stack',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice/stack_1',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice/stack_2',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/range/start',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/range/delta',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/range',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/Maximum/x',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/Maximum',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/Minimum',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/iteration_counter',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/Enter',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/Enter_1',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/Enter_2',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/Enter_3',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/Enter_4',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/Merge',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/Merge_1',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/Merge_2',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/Merge_3',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/Merge_4',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/Less/Enter',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/Less',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/Less_1/Enter',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/Less_1',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/LogicalAnd',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/LoopCond',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/Switch',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/Switch_1',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/Switch_2',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/Switch_3',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/Switch_4',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/Identity',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/Identity_1',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/Identity_2',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/Identity_3',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/Identity_4',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/add/y',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/add',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/TensorArrayReadV3/Enter',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/TensorArrayReadV3/Enter_1',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/TensorArrayReadV3',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/GreaterEqual/Enter',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/GreaterEqual',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/sub/x',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/sub/Enter',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/sub',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/Shape',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/sub/x',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/sub',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/random_uniform/min',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/random_uniform/max',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/random_uniform/RandomUniform',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/random_uniform/sub',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/random_uniform/mul',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/random_uniform',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/add',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/Floor',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/truediv',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/mul',\n",
       " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/shape',\n",
       " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/min',\n",
       " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/max',\n",
       " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/RandomUniform',\n",
       " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/sub',\n",
       " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/mul',\n",
       " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform',\n",
       " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel',\n",
       " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/IsInitialized/VarIsInitializedOp',\n",
       " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Assign',\n",
       " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Read/ReadVariableOp',\n",
       " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Read/Identity',\n",
       " 'encoder_1/bidirectional_rnn/bw/lstm_cell/bias/Initializer/zeros/shape_as_tensor',\n",
       " 'encoder_1/bidirectional_rnn/bw/lstm_cell/bias/Initializer/zeros/Const',\n",
       " 'encoder_1/bidirectional_rnn/bw/lstm_cell/bias/Initializer/zeros',\n",
       " 'encoder_1/bidirectional_rnn/bw/lstm_cell/bias',\n",
       " 'encoder_1/bidirectional_rnn/bw/lstm_cell/bias/IsInitialized/VarIsInitializedOp',\n",
       " 'encoder_1/bidirectional_rnn/bw/lstm_cell/bias/Assign',\n",
       " 'encoder_1/bidirectional_rnn/bw/lstm_cell/bias/Read/ReadVariableOp',\n",
       " 'encoder_1/bidirectional_rnn/bw/lstm_cell/bias/Read/Identity',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat/axis',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/MatMul/Enter',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/MatMul',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd/Enter',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/Const',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/split/split_dim',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/split',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/add/y',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/add',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/Sigmoid',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/mul',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/Sigmoid_1',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/Tanh',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/mul_1',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/add_1',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/Sigmoid_2',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/Tanh_1',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/mul_2',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/Select/Enter',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/Select',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/Select_1',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/Select_2',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/TensorArrayWrite/TensorArrayWriteV3/Enter',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/TensorArrayWrite/TensorArrayWriteV3',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/add_1/y',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/add_1',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/NextIteration',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/NextIteration_1',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/NextIteration_2',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/NextIteration_3',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/NextIteration_4',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/Exit',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/Exit_1',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/Exit_2',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/Exit_3',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/while/Exit_4',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayStack/TensorArraySizeV3',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayStack/range/start',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayStack/range/delta',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayStack/range',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayStack/TensorArrayGatherV3',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/Const_4',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/Rank_1',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/range_1/start',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/range_1/delta',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/range_1',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/concat_2/values_0',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/concat_2/axis',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/concat_2',\n",
       " 'encoder_1/bidirectional_rnn/bw/bw/transpose_1',\n",
       " 'encoder_1/ReverseSequence',\n",
       " 'concat/axis',\n",
       " 'concat',\n",
       " 'StridedSlice/begin',\n",
       " 'StridedSlice/end',\n",
       " 'StridedSlice/strides',\n",
       " 'StridedSlice',\n",
       " 'Fill/dims',\n",
       " 'Fill/value',\n",
       " 'Fill',\n",
       " 'concat_1/axis',\n",
       " 'concat_1',\n",
       " 'embedding_lookup_1/params_0',\n",
       " 'embedding_lookup_1/axis',\n",
       " 'embedding_lookup_1',\n",
       " 'embedding_lookup_1/Identity',\n",
       " 'decoder_0/DropoutWrapperInit/Const',\n",
       " 'decoder_0/DropoutWrapperInit/Const_1',\n",
       " 'decoder_1/DropoutWrapperInit/Const',\n",
       " 'decoder_1/DropoutWrapperInit/Const_1',\n",
       " 'BahdanauAttention/Shape',\n",
       " 'BahdanauAttention/strided_slice/stack',\n",
       " 'BahdanauAttention/strided_slice/stack_1',\n",
       " 'BahdanauAttention/strided_slice/stack_2',\n",
       " 'BahdanauAttention/strided_slice',\n",
       " 'BahdanauAttention/SequenceMask/Const',\n",
       " 'BahdanauAttention/SequenceMask/Const_1',\n",
       " 'BahdanauAttention/SequenceMask/Range',\n",
       " 'BahdanauAttention/SequenceMask/ExpandDims/dim',\n",
       " 'BahdanauAttention/SequenceMask/ExpandDims',\n",
       " 'BahdanauAttention/SequenceMask/Cast',\n",
       " 'BahdanauAttention/SequenceMask/Less',\n",
       " 'BahdanauAttention/SequenceMask/Cast_1',\n",
       " 'BahdanauAttention/Shape_1',\n",
       " 'BahdanauAttention/strided_slice_1/stack',\n",
       " 'BahdanauAttention/strided_slice_1/stack_1',\n",
       " 'BahdanauAttention/strided_slice_1/stack_2',\n",
       " 'BahdanauAttention/strided_slice_1',\n",
       " 'BahdanauAttention/ones/shape_as_tensor',\n",
       " 'BahdanauAttention/ones/Const',\n",
       " 'BahdanauAttention/ones',\n",
       " 'BahdanauAttention/Shape_2',\n",
       " 'BahdanauAttention/strided_slice_2/stack',\n",
       " 'BahdanauAttention/strided_slice_2/stack_1',\n",
       " 'BahdanauAttention/strided_slice_2/stack_2',\n",
       " 'BahdanauAttention/strided_slice_2',\n",
       " 'BahdanauAttention/assert_equal/Equal',\n",
       " 'BahdanauAttention/assert_equal/Const',\n",
       " 'BahdanauAttention/assert_equal/All',\n",
       " 'BahdanauAttention/assert_equal/Assert/Const',\n",
       " 'BahdanauAttention/assert_equal/Assert/Const_1',\n",
       " 'BahdanauAttention/assert_equal/Assert/Const_2',\n",
       " 'BahdanauAttention/assert_equal/Assert/Const_3',\n",
       " 'BahdanauAttention/assert_equal/Assert/Assert/data_0',\n",
       " 'BahdanauAttention/assert_equal/Assert/Assert/data_1',\n",
       " 'BahdanauAttention/assert_equal/Assert/Assert/data_2',\n",
       " 'BahdanauAttention/assert_equal/Assert/Assert/data_4',\n",
       " 'BahdanauAttention/assert_equal/Assert/Assert',\n",
       " 'BahdanauAttention/Shape_3',\n",
       " 'BahdanauAttention/concat/axis',\n",
       " 'BahdanauAttention/concat',\n",
       " 'BahdanauAttention/Reshape',\n",
       " 'BahdanauAttention/mul',\n",
       " 'memory_layer/kernel/Initializer/random_uniform/shape',\n",
       " 'memory_layer/kernel/Initializer/random_uniform/min',\n",
       " 'memory_layer/kernel/Initializer/random_uniform/max',\n",
       " 'memory_layer/kernel/Initializer/random_uniform/RandomUniform',\n",
       " 'memory_layer/kernel/Initializer/random_uniform/sub',\n",
       " 'memory_layer/kernel/Initializer/random_uniform/mul',\n",
       " 'memory_layer/kernel/Initializer/random_uniform',\n",
       " 'memory_layer/kernel',\n",
       " 'memory_layer/kernel/Assign',\n",
       " 'memory_layer/kernel/read',\n",
       " 'BahdanauAttention/memory_layer/Tensordot/axes',\n",
       " 'BahdanauAttention/memory_layer/Tensordot/free',\n",
       " 'BahdanauAttention/memory_layer/Tensordot/Shape',\n",
       " 'BahdanauAttention/memory_layer/Tensordot/GatherV2/axis',\n",
       " 'BahdanauAttention/memory_layer/Tensordot/GatherV2',\n",
       " 'BahdanauAttention/memory_layer/Tensordot/GatherV2_1/axis',\n",
       " 'BahdanauAttention/memory_layer/Tensordot/GatherV2_1',\n",
       " 'BahdanauAttention/memory_layer/Tensordot/Const',\n",
       " 'BahdanauAttention/memory_layer/Tensordot/Prod',\n",
       " 'BahdanauAttention/memory_layer/Tensordot/Const_1',\n",
       " 'BahdanauAttention/memory_layer/Tensordot/Prod_1',\n",
       " 'BahdanauAttention/memory_layer/Tensordot/concat/axis',\n",
       " 'BahdanauAttention/memory_layer/Tensordot/concat',\n",
       " 'BahdanauAttention/memory_layer/Tensordot/stack',\n",
       " 'BahdanauAttention/memory_layer/Tensordot/transpose',\n",
       " 'BahdanauAttention/memory_layer/Tensordot/Reshape',\n",
       " 'BahdanauAttention/memory_layer/Tensordot/transpose_1/perm',\n",
       " 'BahdanauAttention/memory_layer/Tensordot/transpose_1',\n",
       " 'BahdanauAttention/memory_layer/Tensordot/Reshape_1/shape',\n",
       " 'BahdanauAttention/memory_layer/Tensordot/Reshape_1',\n",
       " 'BahdanauAttention/memory_layer/Tensordot/MatMul',\n",
       " 'BahdanauAttention/memory_layer/Tensordot/Const_2',\n",
       " 'BahdanauAttention/memory_layer/Tensordot/concat_1/axis',\n",
       " 'BahdanauAttention/memory_layer/Tensordot/concat_1',\n",
       " 'BahdanauAttention/memory_layer/Tensordot',\n",
       " 'BahdanauAttention/Shape_4',\n",
       " 'BahdanauAttention/strided_slice_3/stack',\n",
       " 'BahdanauAttention/strided_slice_3/stack_1',\n",
       " 'BahdanauAttention/strided_slice_3/stack_2',\n",
       " 'BahdanauAttention/strided_slice_3',\n",
       " 'BahdanauAttention/Shape_5',\n",
       " 'BahdanauAttention/strided_slice_4/stack',\n",
       " 'BahdanauAttention/strided_slice_4/stack_1',\n",
       " 'BahdanauAttention/strided_slice_4/stack_2',\n",
       " 'BahdanauAttention/strided_slice_4',\n",
       " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/Const',\n",
       " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/Const_1',\n",
       " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/concat/axis',\n",
       " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/concat',\n",
       " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/zeros/Const',\n",
       " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/zeros',\n",
       " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/Const_2',\n",
       " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/Const_3',\n",
       " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/Const_4',\n",
       " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/Const_5',\n",
       " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/concat_1/axis',\n",
       " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/concat_1',\n",
       " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1/Const',\n",
       " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1',\n",
       " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/Const_6',\n",
       " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/Const_7',\n",
       " 'AttentionWrapperZeroState/assert_equal/x',\n",
       " 'AttentionWrapperZeroState/assert_equal/Equal',\n",
       " 'AttentionWrapperZeroState/assert_equal/Const',\n",
       " 'AttentionWrapperZeroState/assert_equal/All',\n",
       " 'AttentionWrapperZeroState/assert_equal/Assert/Const',\n",
       " 'AttentionWrapperZeroState/assert_equal/Assert/Const_1',\n",
       " 'AttentionWrapperZeroState/assert_equal/Assert/Const_2',\n",
       " 'AttentionWrapperZeroState/assert_equal/Assert/Const_3',\n",
       " 'AttentionWrapperZeroState/assert_equal/Assert/Assert/data_0',\n",
       " 'AttentionWrapperZeroState/assert_equal/Assert/Assert/data_1',\n",
       " 'AttentionWrapperZeroState/assert_equal/Assert/Assert/data_2',\n",
       " 'AttentionWrapperZeroState/assert_equal/Assert/Assert/data_4',\n",
       " 'AttentionWrapperZeroState/assert_equal/Assert/Assert',\n",
       " 'AttentionWrapperZeroState/checked_cell_state',\n",
       " 'AttentionWrapperZeroState/checked_cell_state_1',\n",
       " 'AttentionWrapperZeroState/Const',\n",
       " 'AttentionWrapperZeroState/ExpandDims/dim',\n",
       " 'AttentionWrapperZeroState/ExpandDims',\n",
       " 'AttentionWrapperZeroState/concat/axis',\n",
       " 'AttentionWrapperZeroState/concat',\n",
       " 'AttentionWrapperZeroState/zeros/Const',\n",
       " 'AttentionWrapperZeroState/zeros',\n",
       " 'AttentionWrapperZeroState/Const_1',\n",
       " 'AttentionWrapperZeroState/ExpandDims_1/dim',\n",
       " 'AttentionWrapperZeroState/ExpandDims_1',\n",
       " 'AttentionWrapperZeroState/zeros_1',\n",
       " 'AttentionWrapperZeroState/Const_2',\n",
       " 'AttentionWrapperZeroState/Const_3',\n",
       " 'AttentionWrapperZeroState/concat_1/axis',\n",
       " 'AttentionWrapperZeroState/concat_1',\n",
       " 'AttentionWrapperZeroState/zeros_2/Const',\n",
       " 'AttentionWrapperZeroState/zeros_2',\n",
       " 'AttentionWrapperZeroState/Const_4',\n",
       " 'AttentionWrapperZeroState/Const_5',\n",
       " 'AttentionWrapperZeroState/Const_6',\n",
       " 'AttentionWrapperZeroState/ExpandDims_2/dim',\n",
       " 'AttentionWrapperZeroState/ExpandDims_2',\n",
       " 'AttentionWrapperZeroState/concat_2/axis',\n",
       " 'AttentionWrapperZeroState/concat_2',\n",
       " ...]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checkpoint = 'C:/Users/Giuseppe/Desktop/NLP/backup/best_model.ckpt' \n",
    "checkpoint = 'C:/Users/Giuseppe/Desktop/NLP/best_model.ckpt'\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "    names = []\n",
    "    [names.append(n.name) for n in loaded_graph.as_graph_def().node]\n",
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa8dd57",
   "metadata": {},
   "source": [
    "Creating the summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c9eff157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_seq(text):\n",
    "    '''Prepare the text for the model'''\n",
    "    \n",
    "    text = clean_text(text)\n",
    "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "696f3d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:/Users/Giuseppe/Desktop/NLP/best_model.ckpt\n",
      "Original Text: Mardi 18 juin, à l'occasion d'une commémoration de l'appel du 18 juin à Evreux dans l'Eure, une vingtaine de jeunes engagés au sein du Service national universel (SNU) ou cadets de la défense, ont été victimes de malaises après être restés debout en plein soleil. Ces jeunes, habillés en uniforme du SNU (casquette, polo blanc et pantalon noir), devaient rester immobiles pendant toute la cérémonie sur les marches de l'hôtel de ville, indique France Bleu Normandie. Or la température ressentie a dépassé les 31 degrés mardi. SNU : les soldats de la république - L'Info du Vrai du 18/06 - CANAL+par L'info du vraiAlors que les personnalités officielles dont le maire de la ville poursuivaient leurs discours, ces jeunes ont dû être pris en charge à l'intérieur de la mairie par les pompiers. Un véhicule du SAMU s'est également rendu sur place, ainsi que des effectifs de la police municipale qui ont aidé les jeunes à se réhydrater. Selon le maire Guy Lefrand, \"deux ou trois jeunes\" ont été plus sérieusement touchés. Dans un communiqué relayé par BFMTV, la préfecture de l'Eure évoque un \"léger coup de chaud lié aux températures particulièrement élevées\", mais aussi \"une forme d'émotion liée au caractère solennel de la cérémonie\". L'édile affirme au micro de France Bleu que les victimes ont \"rapidement retrouvé la forme\".\n",
      "\n",
      "Original summary: Rassemblés à l'occasion de la commémoration de l'appel du 18 juin, ces jeunes du Service national universel sont restés longtemps debout en plein soleil.\n",
      "\n",
      "\n",
      "Text\n",
      "  Input Words: mardi 18 juin occasion commémoration appel 18 juin evreux eure vingtaine jeunes engagés sein service national universel snu cadets défense victimes malaises après être restés debout plein soleil jeunes habillés uniforme snu casquette polo blanc pantalon noir devaient rester immobiles pendant toute cérémonie marches hôtel ville indique france bleu normandie or température ressentie a dépassé 31 degrés mardi snu soldats république info vrai 18 06 canal info vraialors personnalités officielles dont maire ville poursuivaient leurs discours jeunes dû être pris charge intérieur mairie pompiers véhicule samu également rendu place ainsi effectifs police municipale aidé jeunes réhydrater selon maire guy lefrand deux trois jeunes plus sérieusement touchés communiqué relayé bfmtv préfecture eure évoque léger coup chaud lié températures particulièrement élevées aussi forme émotion liée caractère solennel cérémonie édile affirme micro france bleu victimes rapidement retrouvé forme\n",
      "\n",
      "Summary\n",
      "  Response Words: les familles ont été placés en place à la mairie de paris a été placé en place à la mairie de paris\n"
     ]
    }
   ],
   "source": [
    "# Create your own review or use one from the dataset\n",
    "#input_sentence = \"I have never eaten an apple before, but this red one was nice. \\\n",
    "                  #I think that I will try a green apple next time.\"\n",
    "#text = text_to_seq(input_sentence)\n",
    "random = np.random.randint(0,len(clean_input))\n",
    "input_sentence = clean_input[random]\n",
    "text = text_to_seq(clean_input[random])\n",
    "\n",
    "checkpoint = 'C:/Users/Giuseppe/Desktop/NLP/best_model.ckpt'\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
    "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    \n",
    "    #Multiply by batch_size to match the model's input parameters\n",
    "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                      summary_length: [np.random.randint(5,50)], \n",
    "                                      text_length: [len(text)]*batch_size,\n",
    "                                      keep_prob: 1.0})[0] \n",
    "\n",
    "# Remove the padding from the tweet\n",
    "pad = vocab_to_int[\"<PAD>\"] \n",
    "\n",
    "print('Original Text:', df_train_OS.input[random])\n",
    "print('Original summary:', df_train_OS.target[random])#clean_summaries[random]\n",
    "\n",
    "print('\\nText')\n",
    "#print('  Word Ids:    {}'.format([i for i in text]))\n",
    "print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
    "\n",
    "print('\\nSummary')\n",
    "#print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
    "print('  Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
